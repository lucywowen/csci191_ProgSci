{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47125c9",
   "metadata": {},
   "source": [
    "# scikit‑learn: Linear & Polynomial Regression using cross validation\n",
    "\n",
    "This notebook extends on the simple linear regression notebook:\n",
    "- **Linear regression** on a simple dataset (A)  \n",
    "- **Hold‑out train/test split** to test generalization (test set never used during model selection)  \n",
    "- **Cross‑validation (CV)** to compare models **on the training set** only  \n",
    "- **Polynomial regression** on dataset (B) using a `Pipeline(PolynomialFeatures → StandardScaler → Ridge)`\n",
    "\n",
    "**Data files (already included):**\n",
    "- `data/dataset_a.csv` — roughly linear  \n",
    "- `data/dataset_b.csv` — needs polynomial features\n",
    "\n",
    "> Tip: keep this notebook and the `data/` folder in the same directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eabc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "def load_xy(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    X = df[['x']].values  # shape (n, 1)\n",
    "    y = df['y'].values\n",
    "    return df, X, y\n",
    "\n",
    "def report_metrics(name, y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2  = r2_score(y_true, y_pred)\n",
    "    print(f\"{name:>10} | MSE = {mse:.3f}, MAE = {mae:.3f}, R^2 = {r2:.3f}\")\n",
    "    return dict(mse=mse, mae=mae, r2=r2)\n",
    "\n",
    "def plot_fit(ax, X, y, model, title):\n",
    "    ax.scatter(X.ravel(), y, alpha=0.8)\n",
    "    xs = np.linspace(X.min()-0.5, X.max()+0.5, 300).reshape(-1, 1)\n",
    "    ys = model.predict(xs)\n",
    "    ax.plot(xs, ys)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_title(title)\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c9f4b",
   "metadata": {},
   "source": [
    "## Part 1 — Dataset A (Linear)\n",
    "\n",
    "1. Split the data into **train** and **test** (e.g., 80/20).  \n",
    "2. Fit `LinearRegression` on the **training** data.  \n",
    "3. Use **5‑fold CV** (on the training set) to estimate training‑time performance.  \n",
    "4. Evaluate the single final model on the **held‑out test** set to estimate generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bfb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset A\n",
    "df_a, X_a, y_a = load_xy(DATA_DIR / 'dataset_a.csv')\n",
    "df_a.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8601d690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Train/test split\n",
    "Xtr, Xte, ytr, yte = train_test_split(X_a, y_a, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2) Fit Linear Regression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(Xtr, ytr)\n",
    "\n",
    "# 3) Cross‑validation on the *training* split only\n",
    "cv_scores = cross_val_score(lin_reg, Xtr, ytr, scoring='neg_mean_squared_error', cv=5)\n",
    "cv_rmse = (-cv_scores) ** 0.5\n",
    "print(f\"CV RMSE (mean ± std): {cv_rmse.mean():.3f} ± {cv_rmse.std():.3f}\")\n",
    "\n",
    "# 4) Evaluate on train and held‑out test\n",
    "yhat_tr = lin_reg.predict(Xtr)\n",
    "yhat_te = lin_reg.predict(Xte)\n",
    "\n",
    "train_metrics = report_metrics(\"train\", ytr, yhat_tr)\n",
    "test_metrics  = report_metrics(\"   test\", yte, yhat_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e2a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fit on full data for visualization\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "plot_fit(ax, X_a, y_a, lin_reg, \"Dataset A — Linear Regression fit\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae492a95",
   "metadata": {},
   "source": [
    "**Connect to lecture slides:**  \n",
    "- Multiple linear regression notation and vectorization (Lecture 5).  \n",
    "- We'll keep one feature here for clarity and match the straight‑line model.\n",
    "\n",
    "Later we’ll mirror the **Polynomial Regression** slides and include **feature scaling** for stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ef297",
   "metadata": {},
   "source": [
    "### Why both CV *and* a held‑out test set?\n",
    "\n",
    "- Use **K‑fold Cross‑Validation on the *training*** data to choose/compare models (e.g., degree of polynomial, regularization).  \n",
    "- Keep a **separate test set**, untouched during CV/model selection, to evaluate **generalization** once at the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348073d0",
   "metadata": {},
   "source": [
    "## Part 2 — Dataset B (Polynomial features + regularization)\n",
    "\n",
    "We’ll search over polynomial **degree** and Ridge **alpha** using CV on the training split,\n",
    "then report performance on the held‑out test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5260fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset B\n",
    "df_b, X_b, y_b = load_xy(DATA_DIR / 'dataset_b.csv')\n",
    "df_b.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3220ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(X_b, y_b, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a pipeline: PolynomialFeatures -> StandardScaler -> Ridge\n",
    "pipe = Pipeline([\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reg', Ridge())\n",
    "])\n",
    "\n",
    "# Hyperparameter grid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'poly__degree': [1, 2, 3, 4, 5, 6],\n",
    "    'reg__alpha': [0.0, 0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "gcv = GridSearchCV(\n",
    "    pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=None, refit=True\n",
    ")\n",
    "gcv.fit(Xtr_b, ytr_b)\n",
    "\n",
    "print(\"Best params:\", gcv.best_params_)\n",
    "print(\"CV RMSE (best):\", (-gcv.best_score_) ** 0.5)\n",
    "\n",
    "best_model = gcv.best_estimator_\n",
    "\n",
    "# Evaluate on train and held‑out test\n",
    "yhat_tr_b = best_model.predict(Xtr_b)\n",
    "yhat_te_b = best_model.predict(Xte_b)\n",
    "\n",
    "train_metrics_b = report_metrics(\"train\", ytr_b, yhat_tr_b)\n",
    "test_metrics_b  = report_metrics(\"   test\", yte_b, yhat_te_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf915f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the best polynomial fit\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "plot_fit(ax, X_b, y_b, gcv.best_estimator_, f\"Dataset B — Best fit (degree={gcv.best_params_['poly__degree']}, alpha={gcv.best_params_['reg__alpha']})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf8f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how CV error changes with degree (alpha fixed at the best alpha)\n",
    "import numpy as np\n",
    "results = pd.DataFrame(gcv.cv_results_)\n",
    "\n",
    "# Find row with best alpha per degree (lowest mean test MSE)\n",
    "summary = (results\n",
    "           .assign(degree=results['param_poly__degree'].astype(int),\n",
    "                   alpha=results['param_reg__alpha'].astype(float),\n",
    "                   rmse=np.sqrt(-results['mean_test_score'].values))\n",
    "           .sort_values(['degree','rmse'])\n",
    "          )\n",
    "\n",
    "best_alpha = gcv.best_params_['reg__alpha']\n",
    "\n",
    "deg_grid = sorted(summary['degree'].unique())\n",
    "deg_rmse = [summary.query('degree==@d and alpha==@best_alpha')['rmse'].min() for d in deg_grid]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(deg_grid, deg_rmse, marker='o')\n",
    "ax.set_xlabel('Polynomial degree')\n",
    "ax.set_ylabel('CV RMSE (alpha fixed at best)')\n",
    "ax.set_title('Cross‑validated error vs. model complexity (training only)')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
