{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIRPxB3ZfXSIQUAp1vXOEa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucywowen/csci191_ProgSci/blob/main/workshops/debugging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debugging\n",
        "\n",
        "Once testing has uncovered problems, the next step is to fix them. Many novices do this by making more-or-less random changes to their code until it seems to produce the right answer, but that’s very inefficient (and the result is usually only correct for the one case they’re testing). The more experienced a programmer is, the more systematically they debug, and most follow some variation on the rules explained below."
      ],
      "metadata": {
        "id": "GYz4TxLgRusT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Know what it's supposed to do\n",
        "\n",
        "The first step in debugging something is to know what it’s supposed to do. “My program doesn’t work” isn’t good enough: in order to diagnose and fix problems, we need to be able to tell correct output from incorrect. If we can write a test case for the failing case — i.e., if we can assert that with these inputs, the function should produce that result — then we’re ready to start debugging. If we can’t, then we need to figure out how we’re going to know when we’ve fixed things.\n",
        "\n",
        "But writing test cases for scientific software is frequently harder than writing test cases for commercial applications, because if we knew what the output of the scientific code was supposed to be, we wouldn’t be running the software: we’d be writing up our results and moving on to the next program. In practice, scientists tend to do the following:\n",
        "\n",
        "1. Test with simplified data. Before doing statistics on a real data set, we should try calculating statistics for a single record, for two identical records, for two records whose values are one step apart, or for some other case where we can calculate the right answer by hand.\n",
        "\n",
        "2. Test a simplified case. If our program is supposed to simulate magnetic eddies in rapidly-rotating blobs of supercooled helium, our first test should be a blob of helium that isn’t rotating, and isn’t being subjected to any external electromagnetic fields. Similarly, if we’re looking at the effects of climate change on speciation, our first test should hold temperature, precipitation, and other factors constant.\n",
        "\n",
        "3. Compare to an oracle. A test oracle is something whose results are trusted, such as experimental data, an older program, or a human expert. We use test oracles to determine if our new program produces the correct results. If we have a test oracle, we should store its output for particular cases so that we can compare it with our new results as often as we like without re-running that program.\n",
        "\n",
        "4. Check conservation laws. Mass, energy, and other quantities are conserved in physical systems, so they should be in programs as well. Similarly, if we are analyzing patient data, the number of records should either stay the same or decrease as we move from one analysis to the next (since we might throw away outliers or records with missing values). If “new” patients start appearing out of nowhere as we move through our pipeline, it’s probably a sign that something is wrong.\n",
        "\n",
        "5. Visualize. Data analysts frequently use simple visualizations to check both the science they’re doing and the correctness of their code (just as we did in the opening lesson of this tutorial). This should only be used for debugging as a last resort, though, since it’s very hard to compare two visualizations automatically.\n",
        "\n"
      ],
      "metadata": {
        "id": "HUg3WykYTIWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make it fail every time\n",
        "\n",
        "\n",
        "We can only debug something when it fails, so the second step is always to find a test case that makes it fail every time. The “every time” part is important because few things are more frustrating than debugging an intermittent problem: if we have to call a function a dozen times to get a single failure, the odds are good that we’ll scroll past the failure when it actually occurs.\n",
        "\n",
        "As part of this, it’s always important to check that our code is “plugged in”, i.e., that we’re actually exercising the problem that we think we are. Every programmer has spent hours chasing a bug, only to realize that they were actually calling their code on the wrong data set or with the wrong configuration parameters, or are using the wrong version of the software entirely. Mistakes like these are particularly likely to happen when we’re tired, frustrated, and up against a deadline, which is one of the reasons late-night (or overnight) coding sessions are almost never worthwhile.\n"
      ],
      "metadata": {
        "id": "Dxx0_VTlTFdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Make it fail fast\n",
        "\n",
        "If it takes 20 minutes for the bug to surface, we can only do three experiments an hour. This means that we’ll get less data in more time and that we’re more likely to be distracted by other things as we wait for our program to fail, which means the time we are spending on the problem is less focused. It’s therefore critical to make it fail fast.\n",
        "\n",
        "As well as making the program fail fast in time, we want to make it fail fast in space, i.e., we want to localize the failure to the smallest possible region of code:\n",
        "\n",
        "1. The smaller the gap between cause and effect, the easier the connection is to find. Many programmers therefore use a divide and conquer strategy to find bugs, i.e., if the output of a function is wrong, they check whether things are OK in the middle, then concentrate on either the first or second half, and so on.\n",
        "\n",
        "2. N things can interact in N! different ways, so every line of code that isn’t run as part of a test means more than one thing we don’t need to worry about.\n",
        "\n"
      ],
      "metadata": {
        "id": "pF1degF_TDd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change one thing at a time, for a reason\n",
        "Replacing random chunks of code is unlikely to do much good. (After all, if you got it wrong the first time, you’ll probably get it wrong the second and third as well.) Good programmers therefore change one thing at a time, for a reason. They are either trying to gather more information (“is the bug still there if we change the order of the loops?”) or test a fix (“can we make the bug go away by sorting our data before processing it?”).\n",
        "\n",
        "Every time we make a change, however small, we should re-run our tests immediately, because the more things we change at once, the harder it is to know what’s responsible for what (those N! interactions again). And we should re-run all of our tests: more than half of fixes made to code introduce (or re-introduce) bugs, so re-running all of our tests tells us whether we have regressed."
      ],
      "metadata": {
        "id": "sm1EBGtITBe-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keep track of what you've done\n",
        "\n",
        "Good scientists keep track of what they’ve done so that they can reproduce their work, and so that they don’t waste time repeating the same experiments or running ones whose results won’t be interesting. Similarly, debugging works best when we keep track of what we’ve done and how well it worked. If we find ourselves asking, “Did left followed by right with an odd number of lines cause the crash? Or was it right followed by left? Or was I using an even number of lines?” then it’s time to step away from the computer, take a deep breath, and start working more systematically.\n",
        "\n",
        "Records are particularly useful when the time comes to ask for help. People are more likely to listen to us when we can explain clearly what we did, and we’re better able to give them the information they need to be useful."
      ],
      "metadata": {
        "id": "TWlZcCUlTPI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version Control Revisited\n",
        "\n",
        "Version control is often used to reset software to a known state during debugging, and to explore recent changes to code that might be responsible for bugs. In particular, most version control systems (e.g. git) have:\n",
        "\n",
        "- a `blame` command that shows who last changed each line of a file;\n",
        "- a `bisect` command that helps with finding the commit that introduced an issue.\n",
        "\n",
        "We'll talk more about this during our git tutorial!"
      ],
      "metadata": {
        "id": "R--tmRUbTU-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Be humble\n",
        "\n",
        "And speaking of help: if we can’t find a bug in 10 minutes, we should be humble and ask for help. Explaining the problem to someone else is often useful, since hearing what we’re thinking helps us spot inconsistencies and hidden assumptions. If you don’t have someone nearby to share your problem description with, get a rubber duck!\n",
        "\n",
        "Asking for help also helps alleviate confirmation bias. If we have just spent an hour writing a complicated program, we want it to work, so we’re likely to keep telling ourselves why it should, rather than searching for the reason it doesn’t. People who aren’t emotionally invested in the code can be more objective, which is why they’re often able to spot the simple mistakes we have overlooked.\n",
        "\n",
        "Part of being humble is learning from our mistakes. Programmers tend to get the same things wrong over and over: either they don’t understand the language and libraries they’re working with, or their model of how things work is wrong. In either case, taking note of why the error occurred and checking for it next time quickly turns into not making the mistake at all.\n",
        "\n",
        "And that is what makes us most productive in the long run. As the saying goes, A week of hard work can sometimes save you an hour of thought. If we train ourselves to avoid making some kinds of mistakes, to break our code into modular, testable chunks, and to turn every assumption (or mistake) into an assertion, it will actually take us less time to produce working programs, not more."
      ],
      "metadata": {
        "id": "HSByolXYTkkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Error handling\n",
        "When you know your code may crash, Python includes a set of keywords that enable your program to either fail gracefully or continue executing.\n",
        "\n",
        "The try keyword allows you to define a block of \"risky\" code that you think might crash. Python will execute the block of code in a sort of \"safe mode.\" If a crash is encountered within a try block, the interpreter will back-track to the state of your program prior to the crash, then ignore everything following that line within the try block. Several other keywords may be used in conjunction with try in order to enhance its functionality:\n",
        "\n",
        "The except keyword following a try statement enables you to specify a particular error type (default: any error), and execute the given body of the except statement if (and only if) the specified error was encountered. (An error in Python is also called an Exception.) except statements may occur in succession (similar to elif statements), e.g. to handle different types of errors.\n",
        "The finally keyword defines a block of code that is executed regardless of whether an error has occurred and/or been handled. Unlike else statements in if/elif/else blocks, the finally block is always executed (after the try and/or except blocks).\n",
        "Although except statements encapsulate all possible errors by default, it is good practice to specify which particular errors your code is intended to handle. This can help you sort out whether the errors that were encountered were expected or not:"
      ],
      "metadata": {
        "id": "IfzbZdOkKsin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_int_converter(x):\n",
        "  try:\n",
        "    converted = int(float(x)) #to think about: why do we first need to convert to a float, and then to an integer?\n",
        "  except ValueError:\n",
        "    print('I wasn\\'t sure how to compute this value:', x)\n",
        "    converted = None\n",
        "  except TypeError:\n",
        "    print('I wasn\\'t sure how to handle this datatype:', type(x))\n",
        "    converted = None\n",
        "  finally:\n",
        "    print('Conversion complete:', x, '-->', converted)\n",
        "  return converted"
      ],
      "metadata": {
        "id": "XtxQ47MrKuyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try it:\n",
        "\n",
        "Take a function that you have written today, and introduce a tricky bug. Your function should still run, but will give the wrong output. Switch seats with your neighbor and attempt to debug the bug that they introduced into their function. Which of the principles discussed above did you find helpful?\n"
      ],
      "metadata": {
        "id": "j5DXEpPxTum9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are assisting a researcher with Python code that computes the Body Mass Index (BMI) of patients. The researcher is concerned because all patients seemingly have unusual and identical BMIs, despite having different physiques. BMI is calculated as weight in kilograms divided by the square of height in metres.\n",
        "\n",
        "Use the debugging principles in this exercise and locate problems with the code. What suggestions would you give the researcher for ensuring any later changes they make work correctly? What bugs do you spot?"
      ],
      "metadata": {
        "id": "m7UptIYwTyn-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDK1izjkRuM5",
        "outputId": "7ad8fd9c-ae79-4b16-8dfd-8a83755fcf7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patient's BMI (weight: 70.000000, height: 1.800000) is: 0.000367\n",
            "Patient's BMI (weight: 70.000000, height: 1.800000) is: 0.000367\n",
            "Patient's BMI (weight: 70.000000, height: 1.800000) is: 0.000367\n"
          ]
        }
      ],
      "source": [
        "patients = [[70, 1.8], [80, 1.9], [150, 1.7]]\n",
        "\n",
        "def calculate_bmi(weight, height):\n",
        "    return weight / (height ** 2)\n",
        "\n",
        "for patient in patients:\n",
        "    weight, height = patients[0]\n",
        "    bmi = calculate_bmi(height, weight)\n",
        "    print(\"Patient's BMI (weight: %f, height: %f) is: %f\" % (weight, height, bmi)),"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key points:\n",
        "- Know what code is supposed to do before trying to debug it.\n",
        "- Make it fail every time.\n",
        "- Make it fail fast.\n",
        "- Change one thing at a time, and for a reason.\n",
        "- Keep track of what you’ve done.\n",
        "- Be humble.\n"
      ],
      "metadata": {
        "id": "5Osk9j5tUPdk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yiv2sHERT6VY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}