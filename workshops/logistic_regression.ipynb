{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": { "name": "python3", "display_name": "Python 3", "language": "python" },
    "language_info": { "name": "python" }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Workshop: Logistic Regression (Super Simple)\n",
        "\n",
        "A gentle, hands‑on intro to binary classification that mirrors the class slides and prepares you for the assignment.\n",
        "\n",
        "**You will:**\n",
        "1. Make a tiny **binary classification** dataset (2 features → easy to plot).\n",
        "2. Do a **train/test split** and keep the **test set** out of model selection.\n",
        "3. Build a scikit‑learn **Pipeline**: `StandardScaler` → `LogisticRegression`.\n",
        "4. Use **cross‑validation on the training set** to pick `C` (regularization).\n",
        "5. Evaluate **once** on the **held‑out test set** (generalization).\n",
        "6. Visualize a **decision boundary** and a **ROC curve**.\n",
        "\n",
        "**Template to reuse:**  \n",
        "`split → pipeline (scale + logistic) → CV on train → final test → report metrics/plots`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 0) Setup"]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, confusion_matrix, classification_report,\n",
        "    RocCurveDisplay, roc_auc_score, precision_score, recall_score\n",
        ")\n",
        "\n",
        "np.random.seed(42)\n",
        "print(\"Libraries imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 1) Make a tiny dataset (no files needed)\n", "\n", "We create a 2‑feature dataset so we can **see** the classes and the learned boundary."]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "X, y = make_classification(\n",
        "    n_samples=400, n_features=2, n_redundant=0, n_informative=2,\n",
        "    n_clusters_per_class=1, class_sep=1.6, random_state=42\n",
        ")\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[:,0], X[:,1], c=y)\n",
        "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "plt.title(\"Toy dataset (2 features)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 2) Train/test split (hold out a final test set)\n", "\n", "We split once. **All** model selection is done using **only training data** with cross‑validation."]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "print(\"Train size:\", X_train.shape, \" Test size:\", X_test.shape)\n",
        "print(\"Train class balance:\", np.mean(y_train).round(3),\n",
        "      \"  Test class balance:\", np.mean(y_test).round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Pipeline + cross‑validation on the **training** set\n",
        "\n",
        "- `StandardScaler` puts features on comparable scales.  \n",
        "- `LogisticRegression(C)` controls regularization (smaller `C` ⇒ stronger regularization).  \n",
        "- Pick `C` with **5‑fold CV** inside the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe = Pipeline([\n",
        "    (\"scale\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "param_grid = {\"clf__C\": [0.1, 1.0, 10.0]}\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    pipe, param_grid=param_grid, cv=cv, scoring=\"accuracy\", refit=True\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"CV best accuracy (mean over folds):\", grid.best_score_.round(3))\n",
        "best_model = grid.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 4) Final evaluation on the **held‑out test set** (generalization)"]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Test accuracy:\", round(acc, 3))\n",
        "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 5) Visualize the learned decision boundary"]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Mesh for background probabilities\n",
        "x1_min, x1_max = X[:,0].min()-1, X[:,0].max()+1\n",
        "x2_min, x2_max = X[:,1].min()-1, X[:,1].max()+1\n",
        "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 300),\n",
        "                       np.linspace(x2_min, x2_max, 300))\n",
        "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "\n",
        "proba_grid = best_model.predict_proba(grid)[:,1].reshape(xx1.shape)\n",
        "\n",
        "plt.figure()\n",
        "plt.contourf(xx1, xx2, proba_grid, levels=20, alpha=0.35)\n",
        "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, label=\"train\")\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c=y_test, marker=\"x\", label=\"test\")\n",
        "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "plt.title(\"Decision boundary (probability background)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 6) Probability outputs and ROC curve"]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "y_proba = best_model.predict_proba(X_test)[:,1]\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(\"Test ROC AUC:\", round(auc, 3))\n",
        "\n",
        "plt.figure()\n",
        "RocCurveDisplay.from_predictions(y_test, y_proba)\n",
        "plt.title(\"ROC curve (test set)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## 7) Try different decision thresholds\n", "\n", "By default, 0.5 splits the classes. Try 0.3 and 0.7 to see the precision/recall trade‑off."]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def evaluate_threshold(proba, y_true, t=0.5):\n",
        "    y_hat = (proba >= t).astype(int)\n",
        "    acc = accuracy_score(y_true, y_hat)\n",
        "    prec = precision_score(y_true, y_hat)\n",
        "    rec = recall_score(y_true, y_hat)\n",
        "    print(f\"threshold={t:0.2f}  acc={acc:0.3f}  precision={prec:0.3f}  recall={rec:0.3f}\")\n",
        "\n",
        "for t in [0.3, 0.5, 0.7]:\n",
        "    evaluate_threshold(y_proba, y_test, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wrap‑up\n",
        "\n",
        "- Logistic regression outputs probabilities via the sigmoid; a **threshold** turns those into class labels.  \n",
        "- Use **Pipeline** + **CV on the training set**; evaluate **once** on the **test set** for generalization.  \n",
        "- You saw accuracy, confusion matrix, report, ROC/AUC, and threshold effects.\n"
      ]
    }
  ]
}
